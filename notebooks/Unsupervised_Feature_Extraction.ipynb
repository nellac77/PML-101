{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logo.png'>\n",
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Unsupervised Learning and Feature Extraction](#Unsupervised-Learning-and-Feature-Extraction)\n",
    "\t* [Dimensionality Reduction, Feature Extraction and Manifold Learning](#Dimensionality-Reduction,-Feature-Extraction-and-Manifold-Learning)\n",
    "\t\t* [Principal Component Analysis (PCA)](#Principal-Component-Analysis-%28PCA%29)\n",
    "\t\t\t* [Applying PCA to the cancer dataset for visualization](#Applying-PCA-to-the-cancer-dataset-for-visualization)\n",
    "\t\t* [`PCA`: rotation, *sphere*ing,  and whitening example](#PCA:-rotation,-*sphere*ing,--and-whitening-example)\n",
    "\t\t\t* [Center the data](#Center-the-data)\n",
    "\t\t\t* [PCA Take 1: Rotation from Pricipal (Data) Axes to Standard Axes](#PCA-Take-1:-Rotation-from-Pricipal-%28Data%29-Axes-to-Standard-Axes)\n",
    "\t\t\t* [PCA Take 2:  *Sphere*ing (i.e., Whitening or IID Whitening)](#PCA-Take-2:--*Sphere*ing-%28i.e.,-Whitening-or-IID-Whitening%29)\n",
    "\t\t\t* [PCA Take 3:  On Standardized Data](#PCA-Take-3:--On-Standardized-Data)\n",
    "\t\t* [Another PCA Example](#Another-PCA-Example)\n",
    "\t\t* [Non-Negative Matrix Factorization (NMF)](#Non-Negative-Matrix-Factorization-%28NMF%29)\n",
    "\t\t\t* [Applying NMF to synthetic data](#Applying-NMF-to-synthetic-data)\n",
    "\t\t* [Manifold learning with t-SNE](#Manifold-learning-with-t-SNE)\n",
    "\t* [Independent Component Analysis](#Independent-Component-Analysis)\n",
    "\t\t* [Using a `t` distribution with small degrees of freedom (non-Gaussian)](#Using-a-t-distribution-with-small-degrees-of-freedom-%28non-Gaussian%29)\n",
    "* [Summary](#Summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix working directory\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import panel as pn\n",
    "hv.extension('bokeh')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.interpolation'] = \"none\"\n",
    "np.set_printoptions(precision=3)\n",
    "plt.rcParams['image.cmap'] = \"gray\"\n",
    "\n",
    "import src.mglearn as mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction, Feature Extraction and Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the breast cancer data set. There are many features with varying ranges of values.\n",
    "\n",
    "* What subset are most useful?\n",
    "* Which ones exbhibit colinearity?\n",
    "\n",
    "These can be hard questions to answer. It would be better to reduce the dimensionality of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = cancer.target\n",
    "df['target'] = df['target'].replace({0:'malignant', 1:'benign'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malignant = df['target'] == 'malignant'\n",
    "benign = df['target'] == 'benign'\n",
    "\n",
    "fig, axes = plt.subplots(15, 2, figsize=(12, 22))\n",
    "for c,ax in zip(df.columns.drop('target'), axes.ravel()):\n",
    "    opts = dict(bins=50, alpha=0.5, ax=ax, title=c, yticks=[], density=True)\n",
    "    \n",
    "    df.loc[malignant, c].plot.hist(color='r', **opts)\n",
    "    df.loc[benign, c].plot.hist(color='g', **opts)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods like PCA can simplify a colinear input matrix to a smaller set of representative columns (axes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA finds orthogonal axes that best explain variance in the input matrix. Scikit-learn has two PCA models, [sklearn.decomposition.PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and [sklearn.decomposition.IncrementalPCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html), with `IncrementalPCA` allowing fitting to large input matrices that are subsampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_pca_illustration()\n",
    "plt.suptitle(\"pca_illustration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA to the cancer dataset for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cancer.data)\n",
    "X_scaled = scaler.transform(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# keep the first two principal components of the data\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit PCA model to beast cancer data\n",
    "pca.fit(X_scaled)\n",
    "# transform data onto the first two principal components\n",
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "print(\"Original shape: %s\" % str(X_scaled.shape))\n",
    "print(\"Reduced shape: %s\" % str(X_pca.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = (hv.Points((X_pca[:,0], X_pca[:,1],cancer.target),\n",
    "               kdims=['first component', 'second component'], vdims=['Category'])\n",
    "     .options(color='Category', cmap='Category10', size=10, alpha=0.6, width=800, height=600)\n",
    "    )\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA components are linear combinations of the original features. Each column is represented in both components.\n",
    "\n",
    "* Red: more weight in the first component\n",
    "* Blue: more weight in the second component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet\n",
    "\n",
    "plt.matshow(pca.components_, cmap='coolwarm')\n",
    "plt.yticks([0, 1], [\"first component\", \"second component\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(cancer.feature_names)),\n",
    "           cancer.feature_names, rotation=60, ha='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PCA`: rotation, *sphere*ing,  and whitening example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following series of cells show how preprocessing steps such as subtracting off the mean or calculating a z-score `(X - X.mean() / X.std())` can improve the fit of a PCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_options = dict(size=8, width=800, height=400, padding=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [8,12]\n",
    "cov = [[2,3],[5,8]]\n",
    "data = np.random.multivariate_normal(mean, cov, 100)\n",
    "\n",
    "original = hv.Points(data, label='original').options(**plot_options)\n",
    "original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered = data - data.mean(axis=0)\n",
    "\n",
    "center = hv.Points(centered, label='centered').options(**plot_options, legend_position='bottom_right')\n",
    "p = original * center\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Take 1: Rotation from Pricipal (Data) Axes to Standard Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca.fit(centered)\n",
    "rotated = pca.fit_transform(centered)\n",
    "\n",
    "rot = hv.Points(rotated, label='rotated').options(**plot_options)\n",
    "p = p * rot\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Take 2:  *Sphere*ing (i.e., Whitening or IID Whitening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)\n",
    "\n",
    "pca.fit(centered)\n",
    "sphered = pca.fit_transform(centered)\n",
    "\n",
    "white = hv.Points(sphered, label='whitened').options(**plot_options)\n",
    "p = p * white\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Take 3:  On Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdata = (data-np.mean(data, axis=0)) / np.std(data, axis=0, ddof=1)\n",
    "\n",
    "z = hv.Points(zdata, label='Z-scored').options(**plot_options)\n",
    "p = p * z\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca.fit(zdata)\n",
    "rotated_z = pca.fit_transform(zdata)\n",
    "\n",
    "final = hv.Points(rotated_z, label='final PCA').options(**plot_options)\n",
    "p = p * final\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another PCA Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Scatter Matrix is a good way to look at matrices with colinear variables.  Here the `scatter_matrix` shows 3 of 4 columns of the feature matrix are colinear and thus a good candidate for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [8,12]\n",
    "cov = [[2,3],[5,8]]\n",
    "X, Y = np.random.multivariate_normal(mean, cov, 100).T\n",
    "expanded_data = np.c_[2*X + Y, .5*X+.5*Y, X**2 + Y, np.sqrt(X) + np.sin(Y)]\n",
    "centered = expanded_data - np.mean(expanded_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(centered, columns=[str(i) for i in range(4)])\n",
    "hvplot.scatter_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(centered)\n",
    "\n",
    "fmt = \"Component %d explains %f of variance\"\n",
    "print(\"\\n\".join(fmt % (idx, evr) for idx, evr in enumerate(pca.explained_variance_ratio_)))\n",
    "\n",
    "sphered = pca.fit_transform(centered)\n",
    "hv.Points(sphered, label='PCA').options(**plot_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF is a decomposition method solving the following matrix algebra equality on 3 positive matrices:\n",
    "```python\n",
    "V = W * H\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    " * `V`'s shape is `(n, m)`, \n",
    " * `W`'s shape is `(n, n_components)`, and \n",
    " * `H`'s shape is `(n_components, m)`.\n",
    "\n",
    "[This SIAM conference presentation](https://www.siam.org/meetings/sdm11/park.pdf) provides more background on NMF and comparison to other decomposition methods.\n",
    "\n",
    "NMF with scikit-learn [is described here sklearn.decomposition.NMF](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying NMF to synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_nmf_illustration()\n",
    "plt.suptitle(\"nmf_illustration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold learning with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells show manifold learning with a better fit to the `digits` scikit-learn example data than PCA.  More information on manifold learning can be found in \n",
    "* [this comparison of manifold methods](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html)\n",
    "* [a manifold example with severed sphere](http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py)\n",
    "\n",
    "Note [the full documentation on the manifold module](http://scikit-learn.org/stable/modules/manifold.html#manifold) describes a variety of useful estimators we do not cover in this notebook, such as [LocallyLinearEmbedding](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html) and [SpectralEmbedding](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5),\n",
    "                         subplot_kw={'xticks':(), 'yticks': ()})\n",
    "for ax, img in zip(axes.ravel(), digits.images):\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a PCA model\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(digits.data)\n",
    "# transform the digits data onto the first two principal components\n",
    "digits_pca = pca.transform(digits.data)\n",
    "colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
    "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\",\"#535D8E\"]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\n",
    "plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"first principal component\")\n",
    "plt.ylabel(\"second principal component\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=42)\n",
    "# use fit_transform instead of fit, as TSNE has no transform method:\n",
    "digits_tsne = tsne.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be a better choice than PCA for non-Gaussian distributions.\n",
    "\n",
    "From: http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "def plotTwoCol(data, ax, *args, **kwargs):\n",
    "    ax.plot(data[:,0], data[:,1], *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `t` distribution with small degrees of freedom (non-Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = np.random.standard_t(1.5, size=(1000,2))\n",
    "src[:, 0] *= 2\n",
    "\n",
    "# create observed data as mixture of src\n",
    "mix = np.array([[1, 1],   \n",
    "                [0, 2]])\n",
    "obs = np.dot(src, mix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs[i] = <src[i]*A[0], src[i]*A[1]>\n",
    "print(np.dot(src[0], mix[0,:]), np.dot(src[0], mix[1,:]))\n",
    "print(src[0], \"-->\", obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs /= np.std(obs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,5), sharey=True)\n",
    "plotTwoCol(src/np.std(src,axis=0), ax1, \"b.\")\n",
    "plotTwoCol(obs, ax2, 'r.')\n",
    "\n",
    "ax1.set_title(\"True Sources\")\n",
    "ax2.set_title(\"Observed Data\")\n",
    "\n",
    "ax1.set_ylim(-4, 4)\n",
    "ax1.set_xlim(-4, 4)\n",
    "ax2.set_xlim(-4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()\n",
    "rec_pca = pca.fit_transform(obs) # .transform(X)\n",
    "\n",
    "ica = decomposition.FastICA()\n",
    "rec_ica = ica.fit_transform(obs) # .transform(X)  # Estimate the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,5), sharey=True)\n",
    "plotTwoCol(rec_pca / np.std(rec_pca, axis=0), ax1, 'b.')\n",
    "plotTwoCol(rec_ica / np.std(rec_ica, axis=0), ax2, 'r.')\n",
    "\n",
    "ax1.set_title(\"Recovered via PCA\")\n",
    "ax2.set_title(\"Recovered via ICA\")\n",
    "\n",
    "ax1.set_ylim(-4,4)\n",
    "ax1.set_xlim(-4,4)\n",
    "ax2.set_xlim(-4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes =  plt.subplots(2, 2, figsize=(10,10), sharey=True, sharex=True)\n",
    "plotTwoCol(src/np.std(src,axis=0), axes[0,0], \"b.\")\n",
    "plotTwoCol(obs, axes[0,1], 'r.')\n",
    "\n",
    "axes[0,0].set_title(\"True Sources\")\n",
    "axes[0,1].set_title(\"Observed Data\")\n",
    "\n",
    "plotTwoCol(rec_pca / np.std(rec_pca, axis=0), axes[1,0], 'g.')\n",
    "plotTwoCol(rec_ica / np.std(rec_ica, axis=0), axes[1,1], 'y.')\n",
    "\n",
    "axes[1,0].set_title(\"Recovered via PCA\")\n",
    "axes[1,1].set_title(\"Recovered via ICA\")\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlim(-4,4)\n",
    "    ax.set_ylim(-4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reviewed the following topics in preparation for more advanced topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [Dimensionality Reduction, Feature Extraction and Manifold Learning](#Dimensionality-Reduction,-Feature-Extraction-and-Manifold-Learning)\n",
    " * [Principal Component Analysis (PCA)](#Principal-Component-Analysis-%28PCA%29)\n",
    " * [Non-Negative Matrix Factorization (NMF)](#Non-Negative-Matrix-Factorization-%28NMF%29)\n",
    " * [Manifold learning with t-SNE](#Manifold-learning-with-t-SNE)\n",
    " * [Independent Component Analysis](#Independent-Component-Analysis)\n",
    " * [Exercises](#Exercises)\n",
    " * [Summary](#Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='Unsupervised_Feature_Extraction_Exercises.ipynb' class='btn btn-primary btn-lg'>Exercises</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/copyright.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [module-machine-learning]",
   "language": "python",
   "name": "anaconda-project-module-machine-learning-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
